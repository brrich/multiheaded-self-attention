{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aoZTOFKQDQpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9aed1b-5814-4998-f867-0fbd5d697689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-16 21:29:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-10-16 21:29:02 (32.6 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "import requests\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import re\n",
        "# url = \"https://www.gutenberg.org/files/2701/2701-0.txt\"\n",
        "# text = str(requests.get(url).text)\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # read the data\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "K7E_ZZJmF3oK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This sets up a decode and encode function for the text vocab..\n",
        "\n",
        "parsed_str = re.split(r'(\\s+|[^\\w\\s])', text) #  NICE SIMPLE WORD-BASED TOKENIZER splits into a list of strs by whitespace (included) and by punctuation...\n",
        "\n",
        "unique_chars = sorted(list(set(text)))\n",
        "char2index = {}\n",
        "index2char = {}\n",
        "for i,char in enumerate((unique_chars)):\n",
        "  char2index[char] = i\n",
        "  index2char[i] = char\n",
        "\n",
        "def encode_single_char(char: str) -> int:\n",
        "  return char2index[char]\n",
        "\n",
        "def decode_single_token(token: int) -> str:\n",
        "  return index2char[token]\n",
        "\n",
        "def encode(chars: list[str]) -> list[int]:\n",
        "  return [encode_single_char(char) for char in chars]\n",
        "\n",
        "def decode(tokens: list[int], join=True) -> list[str]:\n",
        "  if not join:\n",
        "    return [decode_single_token(token) for token in tokens]\n",
        "  return \"\".join([decode_single_token(token) for token in tokens])"
      ],
      "metadata": {
        "id": "qIrgM5ZJLJKc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'unique characters in our database: {unique_chars}')\n",
        "encoded_text = encode(list(text))\n",
        "print(f'snippet of encoded text: \\n {encoded_text[:100]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dF9fe3dLlWr",
        "outputId": "3f8f18cf-5e9d-4eb0-b4ff-8323d922ac81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique characters in our database: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "snippet of encoded text: \n",
            " [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Constants\n",
        "CONTEXT_WINDOW = 256 # context window\n",
        "BATCH_SIZE = 64 # concurrent sequences to process at the same time\n",
        "VOCAB_SIZE = len(unique_chars) # number of unique vocabulary\n",
        "EMBED_SIZE = 384 # size of embedding vectors y\n",
        "NUMBER_HEADS = 6 # per transformer block how many attention heads\n",
        "NUMBER_BLOCKS = 6 # how many transformer blocks\n",
        "DROPOUT = 0.2 # what dropout rate to use\n",
        "\n",
        "# Training Constants\n",
        "NUM_EPOCHS = 5000\n",
        "LEARNING_RATE = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "NUM_EVAL_SAMPLES = 100 # how many samples to use when evaling\n",
        "EVAL_EVERY = 500 # evaluate model every time we hit this many epochs."
      ],
      "metadata": {
        "id": "imTXc5vNLqq7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets make some batches.....\n",
        "data = torch.tensor(encoded_text, dtype = torch.long)\n",
        "\n",
        "def split_data(data: Tensor, split_ratio: float = 0.9) -> tuple[Tensor, Tensor]:\n",
        "  split_ind = int(len(data) * 0.9)\n",
        "  train,val = data[:split_ind], data[split_ind:]\n",
        "  return train, val\n",
        "\n",
        "train, val = split_data(data)\n",
        "print(f'we have {len(train)} training tokens and {len(val)} val tokens')\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def sample_batch(data: Tensor) -> tuple[Tensor, Tensor]:\n",
        "  \"\"\" Samples a random batch of data \"\"\"\n",
        "  indices = torch.randint(low = 0, high = len(data)-CONTEXT_WINDOW, size = (BATCH_SIZE,))\n",
        "  X = torch.stack([data[index:index+CONTEXT_WINDOW] for index in indices])\n",
        "  y = torch.stack([data[index+1:index+1+CONTEXT_WINDOW] for index in indices])\n",
        "  X = X.to(device)\n",
        "  y = y.to(device)\n",
        "  return X,y\n",
        "\n",
        "X,y = sample_batch(train)\n",
        "# X,y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXXZEVfAjUfL",
        "outputId": "978e09ea-f6f7-4fe5-cd8d-432ce8bdd58e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we have 1003854 training tokens and 111540 val tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from codecs import xmlcharrefreplace_errors\n",
        "# EXAMPLE BIGRAM MODEL\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  \"\"\" simple implementation of an attention head \"\"\"\n",
        "\n",
        "  def __init__(self, head_size: int) -> None:\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.head_size = head_size\n",
        "\n",
        "    # NOTE: the reason it is embedding_diomension_size to head_size is to downsize to head_size.\n",
        "    # Remember these vectors are in the size of the embeddign dimension.\n",
        "    # they have nothing to do with like the context window size or anythign like that.\n",
        "\n",
        "    self.key = nn.Linear(EMBED_SIZE, head_size, bias = False) # (C,H)\n",
        "    self.query = nn.Linear(EMBED_SIZE, head_size, bias = False) # (C,H)\n",
        "    self.value = nn.Linear(EMBED_SIZE, head_size, bias = False) # (C,H)\n",
        "    self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(CONTEXT_WINDOW, CONTEXT_WINDOW)))\n",
        "    # register buffer creates a set self. param tensor that doesn't backprop\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    # extract the shape, and calculate key, query, and value tensors\n",
        "\n",
        "    B,T,C = X.shape # B,T,C\n",
        "\n",
        "    K = self.key(X) # (B,T,C)\n",
        "    Q = self.query(X) # (B, T, C)\n",
        "    V = self.value(X) # (B, T, C)\n",
        "\n",
        "    # compute attention scores (affinities) with QK^T\n",
        "    d_k = self.head_size\n",
        "    K_T = torch.transpose(K, dim0 = -2, dim1 = -1) # (B,C,T)\n",
        "    A = Q@K_T # (B,T,C) @ (B,C,T) = (B,T,T)\n",
        "    A /= (d_k)**(0.5) # scale by sqrt d_k\n",
        "\n",
        "    # apply a mask to the attention scores, then take the softmax\n",
        "    INF = float('inf')\n",
        "    A = A.masked_fill(self.tril[:T, :T] == 0, -INF) # (B, T, T)\n",
        "    A = torch.softmax(A, dim=-1) # (B,T,T)\n",
        "    A = self.dropout(A)\n",
        "\n",
        "    return A @ V # (B,T,T) @ (B,T,C) = @ (B,T,C), same as input dimension\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "\n",
        "  \"\"\" multiheaded self attention layer \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads: int, head_size: int) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(EMBED_SIZE, EMBED_SIZE) # additional projection to \"mix\"\n",
        "    self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    out = torch.cat([head(X) for head in self.heads], dim = -1)\n",
        "    out = self.dropout(out)\n",
        "\n",
        "    return self.proj(out)\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  \"\"\" basic feed forward network \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(EMBED_SIZE, 4*EMBED_SIZE),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*EMBED_SIZE, EMBED_SIZE),\n",
        "        nn.Dropout(DROPOUT)\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.layers(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  \"\"\" MHSA block \"\"\"\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.head_size = EMBED_SIZE // NUMBER_HEADS\n",
        "    self.multihead = MultiHead(NUMBER_HEADS, self.head_size)\n",
        "    self.ffward = FeedForward()\n",
        "    self.ln1 = nn.LayerNorm(EMBED_SIZE)\n",
        "    self.ln2 = nn.LayerNorm(EMBED_SIZE)\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    # include residual connections\n",
        "    x = X + self.multihead(self.ln1(X))\n",
        "    x = x + self.ffward(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.loss_fcn = nn.CrossEntropyLoss() # standard multiclass-classification loss\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(VOCAB_SIZE, EMBED_SIZE, device = device)\n",
        "    self.position_embedding_table = nn.Embedding(CONTEXT_WINDOW, EMBED_SIZE, device = device)\n",
        "\n",
        "    blocks = [Block() for _ in range(NUMBER_BLOCKS)]\n",
        "    self.blocks = nn.Sequential(\n",
        "        *blocks, nn.LayerNorm(EMBED_SIZE)\n",
        "    )\n",
        "\n",
        "    self.lm_head = nn.Linear(in_features = EMBED_SIZE, out_features = VOCAB_SIZE, device = device)\n",
        "    # this maps from the embedding size to the vocab size\n",
        "\n",
        "  def forward(self, X, y = None):\n",
        "\n",
        "    \"\"\" forward pass, calculates loss if applicable X is a (B,T) \"\"\"\n",
        "    B,T = X.shape\n",
        "\n",
        "    tok_embed = self.token_embedding_table(X) # (B,T,C)\n",
        "    pos = torch.stack([torch.arange(T, device = device) for _ in range(B)]) # (B,T,1), ints from 0->CONTEXT_WINDOW-1\n",
        "    pos_embed = self.position_embedding_table(pos) # (B,T,C), we embed these position integers\n",
        "    x = self.blocks(tok_embed + pos_embed)\n",
        "    logits = self.lm_head(x) # (B,T,VOCAB_SIZE)\n",
        "\n",
        "    B,T,C = logits.shape\n",
        "    logits_reshaped = logits.view(B*T, C) # reshape for loss fcn\n",
        "    y = y.view(B*T) if y is not None else None # reshape for loss fcn\n",
        "    return logits, self.loss_fcn(logits_reshaped, y) if y is not None else None\n",
        "\n",
        "\n",
        "  def generate(self, X, max_new_tokens = 100):\n",
        "\n",
        "    \"\"\" generates a new sequence from an existing sequence. \"\"\"\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      X_adjusted = X[:, -CONTEXT_WINDOW:] # this only uses at most CONTEXT_WINDOW of context...\n",
        "\n",
        "      logits, _ = self(X_adjusted)\n",
        "      # select the last element only\n",
        "      logits = logits[:, -1, :]\n",
        "      # get probabilities\n",
        "      proba = torch.softmax(logits, dim=-1)\n",
        "      # sample from the probabilities\n",
        "      X_next = torch.multinomial(proba, num_samples = 1)\n",
        "      X = torch.cat((X,X_next), dim = 1)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "model = LanguageModel()\n",
        "model = model.to(device)\n",
        "logits, loss = model(X,y)\n",
        "\n",
        "start_X = torch.zeros((1,1), dtype = torch.long, device = device)\n",
        "decoded_generation = model.generate(start_X, max_new_tokens=10)[0].tolist()\n",
        "print(decode(decoded_generation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igq3Yqq3UFmx",
        "outputId": "4d33428b-3c0f-44ab-b25d-edc5fd5ab62c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BSAyWzI'S.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## simple training loop\n",
        "def eval(model, data, n_samples = NUM_EVAL_SAMPLES):\n",
        "  \"\"\" slightly better eval function that samples multiple times to get a better idea of loss... \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    losses = []\n",
        "    for _ in range(n_samples):\n",
        "      X,y = sample_batch(data)\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      _, loss = model(X,y)\n",
        "      loss = loss.item()\n",
        "      losses.append(loss)\n",
        "\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "def print_val_and_train_eval(model):\n",
        "  training_loss = eval(model, train)\n",
        "  validation_loss = eval(model, val)\n",
        "  print(f'Epoch {e}/{NUM_EPOCHS}. training loss: {training_loss:.4f}, validation loss: {validation_loss:.4f}')\n",
        "\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  X,y = sample_batch(train)\n",
        "  X = X.to(device)\n",
        "  y = y.to(device)\n",
        "\n",
        "  logits, loss = model(X,y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if e % EVAL_EVERY == 0:\n",
        "    print_val_and_train_eval(model)\n",
        "\n",
        "print_val_and_train_eval(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1wi8XlAfJUi",
        "outputId": "c71880bb-f82d-48c6-bc0c-80f27b81d302"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5000. training loss: 3.9104, validation loss: 3.9244\n",
            "Epoch 500/5000. training loss: 2.3587, validation loss: 2.3841\n",
            "Epoch 1000/5000. training loss: 2.0014, validation loss: 2.0741\n",
            "Epoch 1500/5000. training loss: 1.7673, validation loss: 1.9077\n",
            "Epoch 2000/5000. training loss: 1.6255, validation loss: 1.7876\n",
            "Epoch 2500/5000. training loss: 1.5368, validation loss: 1.7140\n",
            "Epoch 3000/5000. training loss: 1.4686, validation loss: 1.6636\n",
            "Epoch 3500/5000. training loss: 1.4201, validation loss: 1.6221\n",
            "Epoch 4000/5000. training loss: 1.3781, validation loss: 1.5829\n",
            "Epoch 4500/5000. training loss: 1.3408, validation loss: 1.5592\n",
            "Epoch 4999/5000. training loss: 1.3091, validation loss: 1.5398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_X = torch.zeros((1,1), dtype = torch.long, device = device)\n",
        "decoded_generation = model.generate(start_X, max_new_tokens=3000)[0].tolist()\n",
        "print(decode(decoded_generation))"
      ],
      "metadata": {
        "id": "1BDLahhufhKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d51551-8427-43f0-a5c4-90a310b99c82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KING HENRY VI:\n",
            "Cousin, sir, Try Duke of Play thy Duke of York.\n",
            "\n",
            "KING RICHARD II:\n",
            "Pardon, that, I will not to Lancaster;\n",
            "I pray your grame and please to says him,\n",
            "Did case your sure name a sweet love.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "How fled it me speeds to my found\n",
            "Fear, from known obsence wequants, revengrents,\n",
            "Who or hath might the havy of you do not\n",
            "Inter her.\n",
            "\n",
            "LUCIO:\n",
            "Will say yourself gone.\n",
            "\n",
            "DUKE OF YORK:\n",
            "But it you rlead! he your voices,\n",
            "Mafe that your frieny vantage,--\n",
            "\n",
            "First Our Lord:\n",
            "I do never that time Dord Ceitolanus, when\n",
            "The biar, that voice the was to came is to the\n",
            "three thy confetch'd, be that I'll great thee go scorn'd:\n",
            "That tell this asshall I throught me of York and\n",
            "Walt I do inking from the royalous togue of nature\n",
            "With tempt a fremallaring this ination,\n",
            "Which if this tempere us town in my to king.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "By the marriage from of your friends within the kinght,\n",
            "And I have your sentented to safe,\n",
            "And she with knows you do you shall drawn to your prisoner.\n",
            "He would better a  voices be with matity:\n",
            "And of sudden a will right, and tell the brayed\n",
            "Rone aWhich married for execution race that\n",
            "Or posterble to queen doth the eyes. Edward Shall be king's\n",
            "Should prophever'd them to ence, brother man us:\n",
            "Such as or ready to a ignoral godden rewarritable.\n",
            "\n",
            "MARWILIUS:\n",
            "To follow Wert did our that ne'er is powers,\n",
            "'Twas thou me shalt is maid praison of war.\n",
            "\n",
            "CORIOLANUS:\n",
            "Be of God's word,\n",
            "She so bear'd friends the black visance,\n",
            "Wert be that painst of our power's greets;\n",
            "Let for what do seem thereof all that hath\n",
            "To let or one did--\n",
            "And not art to true, and they sir, in the breath,\n",
            "By offends vows.\n",
            "\n",
            "PETRUCHIO:\n",
            "Ay, tell I will not her, tell their obey,\n",
            "God Deparance; call'd me him by the king's either'd,\n",
            "But, an this tirtle way right the last of herse\n",
            "Which with safe stare sunch our nature speak's is in\n",
            "Koth friends and speak take of his pawer.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "And that thou curect a husband declow:\n",
            "And straight him of York state of your lands:\n",
            "And let me thou shrend at thou know'st confess,\n",
            "Like more mine of braw my son, friends, as I deliver,\n",
            "To be custo a gued youth a roak'd a pleasure,\n",
            "And sell it it it graves fortune to be groans.\n",
            "The fier more to of Time into own her with my\n",
            "Richard; musicians your briefs me, my souls perfied\n",
            "Thus nable; why shall be and the stuse me not\n",
            "That ever tommy love to selute his partient.\n",
            "But my heaven to deving again:\n",
            "What is your gracious; but it not sights on mine,\n",
            "But it is con, I have no chifford and it?\n",
            "\n",
            "AUTOLYCUS:\n",
            "Well thou until be done to suckle,\n",
            "As we, it is not as I am a sail,\n",
            "Citizens he as heaven.\n",
            "I speak changer,\n",
            "Not their ever old in the your paintining, he beauted\n",
            "And my earth show'd yet do I more be son.\n",
            "\n",
            "ABFORD:\n",
            "No night.\n",
            "\n",
            "ISABELLA:\n",
            "My son,\n",
            "For not heart the winness for a sright:\n",
            "I am go't How for sweet the batters,\n",
            "That sees are a valet a goes hundroan,\n",
            "And by your villain and a gage to be,\n",
            "As cet her shall to ever scread; then, who, we'll rase arm grant.\n",
            "\n",
            "BUCKINGHAM:\n",
            "So half the cames so have st\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the seed text into token indices\n",
        "seed_text = \"oh yee merry gentlemen\"\n",
        "encoded_seed = encode(list(seed_text))  # list[int]\n",
        "\n",
        "# Put it into a batch of size 1\n",
        "start_X = torch.tensor([encoded_seed], dtype=torch.long, device=device)\n",
        "\n",
        "# Generate continuation\n",
        "generated_tokens = model.generate(start_X, max_new_tokens=300)[0].tolist()\n",
        "generated_text = decode(generated_tokens)\n",
        "\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "AkJSzwU5NdGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3475461-1944-46e8-e62f-71c99608664e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oh yee merry gentlemen;\n",
            "And even the selented to good him foe,\n",
            "May never I in remember e's part.\n",
            "\n",
            "GLOUCESTER:\n",
            "Even that hour's it majesty manac, strew down\n",
            "I breathe thee; I will with it sumer;\n",
            "Therefore I pritheel, not time ends I held\n",
            "The came of standers most but sight.\n",
            "To what near shall touch'd my sweet from a miles\n"
          ]
        }
      ]
    }
  ]
}