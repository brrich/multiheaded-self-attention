# multiheaded-self-attention
A Multiheaded self attention transformer architeture implementation, without using pytorch transformers.
